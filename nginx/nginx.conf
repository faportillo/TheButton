# =============================================================================
# NGINX Configuration for TheButton API
# =============================================================================
# This file configures nginx as a reverse proxy in front of the FastAPI app.
# A reverse proxy sits between clients and your backend, forwarding requests
# and responses. Benefits: load balancing, SSL termination, caching, etc.
# =============================================================================

# -----------------------------------------------------------------------------
# GLOBAL DIRECTIVES (outside http block)
# -----------------------------------------------------------------------------

# User and group that nginx worker processes run as
# Security: run as non-root user (nginx user is created by the nginx image)
user nginx;

# Number of worker processes (auto = number of CPU cores)
# Each worker can handle many connections, so this is usually optimal
worker_processes auto;

# Error log location and severity level
# Options: debug, info, notice, warn, error, crit, alert, emerg
error_log /var/log/nginx/error.log warn;

# PID file location (stores the master process ID)
pid /var/run/nginx.pid;

# -----------------------------------------------------------------------------
# EVENTS BLOCK
# -----------------------------------------------------------------------------
# Controls how nginx handles connections at the network level
events {
    # Maximum number of simultaneous connections per worker process
    # Total connections = worker_processes Ã— worker_connections
    # With auto workers and 1024 connections, you can handle thousands of clients
    worker_connections 1024;
    
    # Use epoll on Linux (most efficient event model)
    # This is usually auto-detected, but can be explicitly set
    # use epoll;
}

# -----------------------------------------------------------------------------
# HTTP BLOCK
# -----------------------------------------------------------------------------
# All HTTP-related configuration goes here
http {
    # -------------------------------------------------------------------------
    # MIME TYPES
    # -------------------------------------------------------------------------
    # Tells nginx how to handle different file types
    # Includes default mappings (text/html, application/json, etc.)
    include /etc/nginx/mime.types;
    
    # Default MIME type if file type can't be determined
    default_type application/octet-stream;

    # -------------------------------------------------------------------------
    # LOGGING FORMAT
    # -------------------------------------------------------------------------
    # Define custom log format for access logs
    # This creates a format called "main" that we'll use for access logs
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';
    
    # Variables explained:
    #   $remote_addr        - Client IP address
    #   $remote_user        - Username from HTTP Basic Auth (if used)
    #   $time_local         - Local time in Common Log Format
    #   $request            - Full request line (method, URI, protocol)
    #   $status             - Response status code (200, 404, etc.)
    #   $body_bytes_sent    - Bytes sent to client (response size)
    #   $http_referer       - Referrer header (where request came from)
    #   $http_user_agent    - Browser/client identifier
    #   $http_x_forwarded_for - Original client IP (if behind proxy)

    # Access log: log all requests using the "main" format
    access_log /var/log/nginx/access.log main;

    # -------------------------------------------------------------------------
    # PERFORMANCE SETTINGS
    # -------------------------------------------------------------------------
    # Use sendfile() system call for efficient file transfers
    # Instead of reading file into nginx buffer, then sending,
    # sendfile() tells the OS to send file directly from disk to network
    sendfile on;
    
    # Wait for packets to fill up before sending (TCP optimization)
    # Reduces number of network packets, but may add slight latency
    tcp_nopush on;
    
    # Disable Nagle's algorithm (send data immediately, don't wait)
    # Good for real-time apps like SSE where low latency matters
    tcp_nodelay on;
    
    # How long to keep idle connections open
    # Higher = fewer connection establishments, but more memory usage
    keepalive_timeout 65;
    
    # Maximum size of types hash table (for MIME type lookups)
    # Increase if you have many custom MIME types
    types_hash_max_size 2048;

    # -------------------------------------------------------------------------
    # GZIP COMPRESSION
    # -------------------------------------------------------------------------
    # Compress responses before sending to clients
    # Reduces bandwidth usage, speeds up page loads
    gzip on;
    
    # Add "Vary: Accept-Encoding" header to indicate compression
    # Tells caches that response varies based on client's Accept-Encoding
    gzip_vary on;
    
    # Compress responses even for proxied requests
    # "any" means compress all proxied responses
    gzip_proxied any;
    
    # Compression level (1-9, higher = more compression but more CPU)
    # 6 is a good balance
    gzip_comp_level 6;
    
    # Only compress these MIME types
    # Don't compress images (they're already compressed) or binary files
    gzip_types 
        text/plain 
        text/css 
        text/xml 
        text/javascript 
        application/json 
        application/javascript 
        application/xml+rss 
        application/rss+xml 
        font/truetype 
        font/opentype 
        application/vnd.ms-fontobject 
        image/svg+xml;

    # -------------------------------------------------------------------------
    # UPSTREAM BLOCK (Backend Server Definition)
    # -------------------------------------------------------------------------
    # Defines the backend server(s) that nginx will proxy to
    # "api_backend" is just a name - you can call it anything
    upstream api_backend {
        # Backend server address
        # "api" is the Docker service name (resolved by Docker's DNS)
        # Port 8000 is where FastAPI is listening
        server api:8000;
        
        # You can add multiple servers for load balancing:
        # server api:8000 weight=3;    # This server gets 3x more requests
        # server api2:8000 weight=1;   # This server gets 1x requests
        # server api3:8000 backup;     # Only used if others are down
        
        # Load balancing methods (default is round-robin):
        # - ip_hash: Route same IP to same server (session stickiness)
        # - least_conn: Send to server with fewest connections
        # - fair: Send to server with fastest response time
    }

    # -------------------------------------------------------------------------
    # SERVER BLOCK (Virtual Host)
    # -------------------------------------------------------------------------
    # Each server block defines how to handle requests for a specific domain/port
    # You can have multiple server blocks for different sites
    server {
        # Listen on port 80 (HTTP) on all network interfaces (0.0.0.0)
        # In Docker, this means the container's port 80
        listen 80;
        
        # Server name (domain name or _ for catch-all)
        # "_" means "match any hostname"
        # In production, you'd use: server_name api.example.com;
        server_name _;

        # ---------------------------------------------------------------------
        # CLIENT SETTINGS
        # ---------------------------------------------------------------------
        # Maximum size of client request body (file uploads, POST data)
        # 10M = 10 megabytes
        # If exceeded, client gets "413 Request Entity Too Large"
        client_max_body_size 10M;

        # ---------------------------------------------------------------------
        # PROXY SETTINGS (How to forward requests to backend)
        # ---------------------------------------------------------------------
        # Use HTTP/1.1 for proxying (required for keepalive connections)
        proxy_http_version 1.1;
        
        # Forward the original Host header to backend
        # Backend needs to know what domain was requested
        proxy_set_header Host $host;
        
        # CRITICAL: Set the real client IP address
        # Without this, your FastAPI app sees nginx's IP, not the client's IP
        # This is essential for rate limiting to work correctly!
        proxy_set_header X-Real-IP $remote_addr;
        
        # Forward the chain of proxy IPs
        # Format: "client_ip, proxy1_ip, proxy2_ip"
        # Your app can extract the first IP to get the original client
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        # Tell backend if original request was HTTPS
        # Important for apps that need to know if connection was secure
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Clear Connection header to allow keepalive
        # Prevents "Connection: close" from breaking keepalive connections
        proxy_set_header Connection "";

        # ---------------------------------------------------------------------
        # TIMEOUT SETTINGS
        # ---------------------------------------------------------------------
        # How long to wait when connecting to backend
        proxy_connect_timeout 60s;
        
        # How long to wait when sending request to backend
        proxy_send_timeout 60s;
        
        # How long to wait for backend response
        # Important for SSE (Server-Sent Events) which are long-lived connections
        # 60s might be too short for SSE - consider increasing if needed
        proxy_read_timeout 60s;

        # ---------------------------------------------------------------------
        # BUFFERING SETTINGS (Critical for SSE!)
        # ---------------------------------------------------------------------
        # Disable response buffering
        # Normally nginx buffers responses and sends when complete
        # For SSE, we need to send data immediately as it arrives
        proxy_buffering off;
        
        # Disable response caching
        # Don't cache responses from backend
        proxy_cache off;

        # ---------------------------------------------------------------------
        # LOCATION BLOCKS (Route different URLs differently)
        # ---------------------------------------------------------------------
        
        # Health check endpoint (optional optimization)
        # Health checks are frequent, so we disable access logging for them
        location /health {
            # Forward to backend
            proxy_pass http://api_backend;
            
            # Don't log health check requests (reduces log noise)
            access_log off;
        }

        # All other requests (catch-all)
        location / {
            # Forward all requests to the backend
            proxy_pass http://api_backend;
            
            # Tell nginx not to buffer this response
            # This header is read by nginx itself (not sent to client)
            # Critical for Server-Sent Events (SSE) streaming
            proxy_set_header X-Accel-Buffering no;
        }
        
        # You could add more location blocks for different routes:
        # location /api/ {
        #     proxy_pass http://api_backend;
        # }
        # location /static/ {
        #     alias /var/www/static/;  # Serve static files directly
        # }
    }
}
